{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets up yaml for Twitter credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config = dict(\n",
    "    search_tweets_api = dict(\n",
    "        account_type = 'premium',\n",
    "        endpoint = 'https://api.twitter.com/1.1/tweets/search/fullarchive/label.json',\n",
    "        consumer_key = '',\n",
    "        consumer_secret = ''\n",
    "    )\n",
    ")\n",
    "\n",
    "with open('twitter_keys_fullarchive.yaml', 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import load_credentials\n",
    "\n",
    "premium_search_args = load_credentials(\"twitter_keys_fullarchive.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "print(premium_search_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishes payload rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import gen_rule_payload\n",
    "hashtag = \"#stockmarket OR #federalreserve\"\n",
    "rule = gen_rule_payload(hashtag, \n",
    "                        results_per_call=100,\n",
    "                        from_date=\"2019-09-25 07:15\",\n",
    "                        to_date=\"2019-12-04 23:11\"\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combines credentials and payload rule \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import ResultStream\n",
    "\n",
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=5000,\n",
    "                  **premium_search_args)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connects with API and loads Tweets into .jsonl file for distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('twitter_premium_api_demo.jsonl', 'a', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for tweet in rs.stream():\n",
    "        n += 1\n",
    "        if n % 100 == 0:\n",
    "            print('{0}: {1}'.format(str(n), tweet['created_at']))\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opens .jsonl file and loads all tweet data into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.DataFrame()\n",
    "\n",
    "with open('twitter_premium_api_demo.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "for json_str in json_list:\n",
    "    result = json_normalize(json.loads(json_str))\n",
    "    json_df = json_df.append(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates new df for full texts from all tweet types and created date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = json_df[['retweeted_status.retweeted',\n",
    "                  'retweeted_status.extended_tweet.full_text',  \n",
    "                  'extended_tweet.full_text', \n",
    "                  'created_at']].copy()\n",
    "\n",
    "new_df['final_tweet'] = np.where(new_df['retweeted_status.retweeted'] == False, new_df['retweeted_status.extended_tweet.full_text'], new_df['extended_tweet.full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines functions for date time strip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_strip(col):\n",
    "    dt_object1 = datetime.datetime.strptime(col, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    return dt_object1\n",
    "\n",
    "def actual_date(col):\n",
    "    match = re.search('\\d{4}-\\d{2}-\\d{2}',str(col))\n",
    "    test = datetime.datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "    return test\n",
    "    \n",
    "    \n",
    "new_df['date_time'] = new_df['created_at'].apply(date_strip).copy()\n",
    "\n",
    "new_df['final_date'] = new_df['date_time'].apply(actual_date).copy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
